from flask import Flask, jsonify, request, send_file, send_from_directory
from flask_cors import CORS
from celery import Celery
import glob
import subprocess
import os
import time
import sqlite3
from youtube_scrapper_for_internship_main.state import set_stop_flag
from youtube_scrapper_for_internship_main.database import DATABASE, export_to_csv
from youtube_scrapper_for_internship_main.utils import split_tables


app = Flask(__name__)
CORS(app)

celery = Celery(app.name, broker='redis://redis:6379/0', backend='redis://redis:6379/0')

def get_db_connection():
    conn = sqlite3.connect(DATABASE)
    conn.row_factory = sqlite3.Row
    return conn

@app.route('/stop_scrape', methods=['POST'])
def stop_scrape():
    set_stop_flag()
    export_to_csv() 
    split_tables()
    return jsonify({"status": "stopping"}), 200

@app.route('/channels', methods=['GET'])
def get_channels():
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM influencers")
    rows = cursor.fetchall()
    conn.close()
    return jsonify([dict(row) for row in rows])


@app.route('/download_csv/<filename>', methods=['GET'])
def download_csv(filename):
    csv_dir = os.path.join(os.path.dirname(__file__), "youtube_scrapper_for_internship_main", "databases")

    if not filename.endswith(".csv") or not os.path.exists(os.path.join(csv_dir, filename)):
        return jsonify({"error": "CSV file not found"}), 404

    return send_from_directory(csv_dir, filename, as_attachment=True)

@app.route('/start_scrape', methods=['POST'])
def start_scrape():
    task = run_scraper.delay()
    return jsonify({"task_id": task.id}), 202

@app.route('/scrape_status/<task_id>', methods=['GET'])
def scrape_status(task_id):
    task = run_scraper.AsyncResult(task_id)
    if task.state == 'PENDING':
        return jsonify({'status': 'pending'}), 200
    elif task.state == 'SUCCESS':
        return jsonify({'status': 'completed',
                        'total_time': task.result.get("total_time")
                        }), 200
    elif task.state == 'FAILURE':
        return jsonify({'status': 'failed', 'error': str(task.result), 'traceback': task.traceback}), 500
    return jsonify({'status': task.state.lower()}), 200

@app.route('/csv_files', methods=['GET'])
def list_csv_files():
    csv_dir = os.path.join(os.path.dirname(__file__), "youtube_scrapper_for_internship_main", "databases")
    try:
        files = [f for f in os.listdir(csv_dir) if f.endswith(".csv")]
        return jsonify(files)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@celery.task
def run_scraper():
    start_time = time.time()  

    for file in glob.glob("youtube_scrapper_for_internship_main/databases/*.csv"):
        os.remove(file)

    try:
        result = subprocess.run(
            ["python", "main.py"],
            cwd="youtube_scrapper_for_internship_main",
            check=True,
            capture_output=True,
            text=True
        )
        print("‚úÖ Scraping finished:", result.stdout)

    except subprocess.CalledProcessError as e:
        print("‚ùå Error while running main.py:", e.stderr)

    finally:
        try:
            from youtube_scrapper_for_internship_main.database import export_to_csv
            from youtube_scrapper_for_internship_main.utils import split_tables
            from youtube_scrapper_for_internship_main.state import clear_stop_flag

            print("üíæ Final CSV export and table splitting...")
            export_to_csv()
            split_tables()
            clear_stop_flag()

        except Exception as ex:
            print("‚ö†Ô∏è Failed to save CSV:", ex)

    end_time = time.time()  
    total_time = end_time - start_time
    print(f"‚è± Total scraping time: {total_time:.2f} seconds")

    return {"status": "done", "total_time": total_time}

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=5001)
